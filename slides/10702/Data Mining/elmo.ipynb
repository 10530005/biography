{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "I0506 14:22:11.548615  4780 __init__.py:111] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MCUCC\\AppData\\Local\\Temp\\jieba.cache\n",
      "I0506 14:22:11.548615  4780 __init__.py:131] Loading model from cache C:\\Users\\MCUCC\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.636 seconds.\n",
      "I0506 14:22:12.184971  4780 __init__.py:163] Loading model cost 0.636 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "I0506 14:22:12.188971  4780 __init__.py:164] Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./train.csv', index_col=0, encoding='utf-8').astype(str)\n",
    "\n",
    "cols = ['title1_zh','title2_zh', 'label']\n",
    "train = train.loc[:, cols]\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "import time\n",
    "\n",
    "def jieba_tokenizer(text):       \n",
    "    words = pseg.cut(text)    \n",
    "    return ' '.join([word for word, flag in words if flag != 'x'])\n",
    "\n",
    "train['title1_tokenized'] = train.loc[:, 'title1_zh'].apply(jieba_tokenizer)\n",
    "train['title2_tokenized'] = train.loc[:, 'title2_zh'].apply(jieba_tokenizer)\n",
    "\n",
    "import keras\n",
    "\n",
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "corpus_x1 = train.title1_tokenized\n",
    "corpus_x2 = train.title2_tokenized\n",
    "corpus = pd.concat([corpus_x1, corpus_x2])\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "x1_train = tokenizer.texts_to_sequences(corpus_x1)\n",
    "x2_train = tokenizer.texts_to_sequences(corpus_x2)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "x1_train = keras.preprocessing.sequence.pad_sequences(x1_train,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x2_train = keras.preprocessing.sequence.pad_sequences(x2_train,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 定義每一個分類對應到的索引數字\n",
    "label_to_index = {\n",
    "    'unrelated': 0, \n",
    "    'agreed': 1, \n",
    "    'disagreed': 2\n",
    "}\n",
    "\n",
    "# 將分類標籤對應到剛定義的數字\n",
    "y_train = train.label.apply(lambda x: label_to_index[x])\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 50\n",
    "x1_train, x1_val, x2_train, x2_val, y_train, y_val = train_test_split(x1_train, x2_train, y_train, test_size = VALIDATION_RATIO, random_state = RANDOM_STATE)\n",
    "\n",
    "test = pd.read_csv('./test.csv', index_col=0, encoding='utf-8')\n",
    "test = test.dropna(how = 'any', axis = 0)\n",
    "\n",
    "# 以下步驟分別對新聞標題 A、B　進行\n",
    "# 文本斷詞 / Word Segmentation\n",
    "test['title1_tokenized'] = test.loc[:, 'title1_zh'].apply(jieba_tokenizer)\n",
    "test['title2_tokenized'] = test.loc[:, 'title2_zh'].apply(jieba_tokenizer)\n",
    "\n",
    "# 將詞彙序列轉為索引數字的序列\n",
    "x1_test = tokenizer.texts_to_sequences(test.title1_tokenized)\n",
    "x2_test = tokenizer.texts_to_sequences(test.title2_tokenized)\n",
    "\n",
    "# 為數字序列加入 zero padding\n",
    "x1_test = keras.preprocessing.sequence.pad_sequences(x1_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x2_test = keras.preprocessing.sequence.pad_sequences(x2_test, maxlen=MAX_SEQUENCE_LENGTH)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 14:39:09.206950  4780 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 14:39:09.494951  4780 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "el_mo_embedding_2 (ELMoEmbeddin (None, 20, 1024)     0           input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 128)          590336      el_mo_embedding_2[0][0]          \n",
      "                                                                 el_mo_embedding_2[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            771         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 591,107\n",
      "Trainable params: 591,107\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 建立孿生 LSTM 架構（Siamese LSTM）\n",
    "import tensorflow as tf\n",
    "from keras import Input\n",
    "from keras.layers import LSTM, concatenate, Dense\n",
    "from keras.models import Model\n",
    "import utils\n",
    "from elmo import ELMoEmbedding\n",
    "\n",
    "# 基本參數設置，有幾個分類\n",
    "NUM_CLASSES = 3\n",
    "# 在語料庫裡有多少詞彙\n",
    "MAX_NUM_WORDS = 10000\n",
    "# 一個標題最長有幾個詞彙\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "# 一個詞向量的維度\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "# LSTM 輸出的向量維度\n",
    "NUM_LSTM_UNITS = 128\n",
    "\n",
    "top_input = Input(shape=(20, ), dtype='int64')\n",
    "bm_input = Input(shape=(20, ), dtype='int64')\n",
    "\n",
    "idx2word = utils.get_idx2word()\n",
    "\n",
    "embedding_layer = ELMoEmbedding(idx2word=idx2word, output_mode=\"elmo\", trainable=True)\n",
    "top_embedded = embedding_layer(top_input)\n",
    "bm_embedded = embedding_layer(bm_input)\n",
    "\n",
    "share_lstm = LSTM(NUM_LSTM_UNITS)\n",
    "top_output = share_lstm(top_embedded)\n",
    "bm_output = share_lstm(bm_embedded)\n",
    "\n",
    "merged = concatenate([top_output, bm_output], axis=-1)\n",
    "\n",
    "# 全連接層搭配 Softmax Activation\n",
    "# 可以回傳 3 個成對標題\n",
    "# 屬於各類別的可能機率\n",
    "dense =  Dense(units=NUM_CLASSES, activation='softmax')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "# 我們的模型就是將數字序列的輸入，轉換\n",
    "# 成 3 個分類的機率的所有步驟 / 層的總和\n",
    "model = Model(inputs=[top_input, bm_input], outputs=predictions)\n",
    "\n",
    "# from keras.utils import plot_model\n",
    "# plot_model(\n",
    "#     model, \n",
    "#     to_file='model.png', \n",
    "#     show_shapes=True, \n",
    "#     show_layer_names=False, \n",
    "#     rankdir='LR')\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\mcucc\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0506 14:39:09.950957  4780 deprecation.py:323] From c:\\users\\mcucc\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 288496 samples, validate on 32056 samples\n",
      "Epoch 1/10\n",
      "288496/288496 [==============================] - 15274s 53ms/step - loss: 0.3541 - acc: 0.8213 - val_loss: 0.3109 - val_acc: 0.8479\n",
      "Epoch 2/10\n",
      "288496/288496 [==============================] - 15261s 53ms/step - loss: 0.2790 - acc: 0.8682 - val_loss: 0.2757 - val_acc: 0.8714\n",
      "Epoch 3/10\n",
      "288496/288496 [==============================] - 15263s 53ms/step - loss: 0.2386 - acc: 0.8932 - val_loss: 0.2601 - val_acc: 0.8839\n",
      "Epoch 4/10\n",
      "288496/288496 [==============================] - 15272s 53ms/step - loss: 0.2100 - acc: 0.9088 - val_loss: 0.2591 - val_acc: 0.8869\n",
      "Epoch 5/10\n",
      "288496/288496 [==============================] - 15261s 53ms/step - loss: 0.1879 - acc: 0.9194 - val_loss: 0.2523 - val_acc: 0.8929\n",
      "Epoch 6/10\n",
      "288496/288496 [==============================] - 15592s 54ms/step - loss: 0.1682 - acc: 0.9294 - val_loss: 0.2547 - val_acc: 0.8947\n",
      "Epoch 7/10\n",
      "288496/288496 [==============================] - 15323s 53ms/step - loss: 0.1528 - acc: 0.9363 - val_loss: 0.2595 - val_acc: 0.8950\n",
      "Epoch 8/10\n",
      "288496/288496 [==============================] - 15273s 53ms/step - loss: 0.1386 - acc: 0.9432 - val_loss: 0.2702 - val_acc: 0.8970\n",
      "Epoch 9/10\n",
      "288496/288496 [==============================] - 15286s 53ms/step - loss: 0.1277 - acc: 0.9480 - val_loss: 0.2746 - val_acc: 0.8947\n",
      "Epoch 10/10\n",
      "288496/288496 [==============================] - 15313s 53ms/step - loss: 0.1178 - acc: 0.9525 - val_loss: 0.2847 - val_acc: 0.8967\n"
     ]
    }
   ],
   "source": [
    "# 實際訓練模型\n",
    "history = model.fit(\n",
    "    # 輸入是兩個長度為 20 的數字序列\n",
    "    x=[x1_train, x2_train], \n",
    "    y=y_train,\n",
    "    batch_size=512,\n",
    "    epochs=10,\n",
    "    # 每個 epoch 完後計算驗證資料集\n",
    "    # 上的 Loss 以及準確度\n",
    "    validation_data=([x1_val, x2_val],y_val),\n",
    "    # 每個 epoch 隨機調整訓練資料集\n",
    "    # 裡頭的數據以讓訓練過程更穩定\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用已訓練的模型做預測\n",
    "\n",
    "predictions = model.predict([x1_test, x2_test], batch_size=20)\n",
    "\n",
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "test['Category'] = [index_to_label[idx] for idx in np.argmax(predictions, axis=1)]\n",
    "\n",
    "submission = test.loc[:, ['Category']].reset_index()\n",
    "\n",
    "submission.columns = ['Id', 'Category']\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
